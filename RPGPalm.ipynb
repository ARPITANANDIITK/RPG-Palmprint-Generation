{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "686d56e3-f2c1-421c-9b64-a57be6560dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c8023-dd80-45a0-a7ee-cc3b566810e6",
   "metadata": {},
   "source": [
    "### Generator (G) Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6601a64-38e6-4f98-97f3-a90914619439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAdaIN(nn.Module):\n",
    "    def __init__(self, input_dim, noise_dim=128):\n",
    "        super(CAdaIN, self).__init__()\n",
    "        self.fc_mean = nn.Linear(noise_dim, input_dim)\n",
    "        self.fc_var = nn.Linear(noise_dim, input_dim)\n",
    "        self.noise_layer = nn.Parameter(torch.zeros(1, input_dim, 1, 1))\n",
    "\n",
    "    def forward(self, x, noise):\n",
    "        mean = self.fc_mean(noise).view(-1, x.size(1), 1, 1)\n",
    "        var = self.fc_var(noise).view(-1, x.size(1), 1, 1)\n",
    "        x_normalized = (x - x.mean(dim=[2, 3], keepdim=True)) / (x.std(dim=[2, 3], keepdim=True) + 1e-5)\n",
    "        return mean * x_normalized + var + self.noise_layer\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, noise_dim=128):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.caadain = CAdaIN(out_channels, noise_dim)\n",
    "\n",
    "    def forward(self, x, noise):\n",
    "        x = self.conv(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.bn(x)\n",
    "        down = self.caadain(x, noise)\n",
    "        return down\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, noise_dim=128):\n",
    "        super(UpBlock, self).__init__()\n",
    "        # ConvTranspose2d for upsampling (preserve size)\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, output_padding=0)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.caadain = CAdaIN(out_channels, noise_dim)\n",
    "        \n",
    "        # Additional convolution to reduce channels after concatenation with skip connection\n",
    "        self.conv = nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, noise, skip_connection):\n",
    "        # Apply deconvolution (up-sampling)\n",
    "        x = self.deconv(x)          \n",
    "        x = torch.cat([x, skip_connection], dim=1) \n",
    "        x = self.conv(x)            \n",
    "        x = self.relu(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.caadain(x, noise)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "231f8aba-4af1-41b6-b017-f07a918dde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Down blocks\n",
    "        self.down1 = DownBlock(64, 128, noise_dim)\n",
    "        self.down2 = DownBlock(128, 256, noise_dim)\n",
    "        self.down3 = DownBlock(256, 256, noise_dim)\n",
    "        self.down4 = DownBlock(256, 256, noise_dim)\n",
    "        self.down5 = DownBlock(256, 256, noise_dim)\n",
    "        self.down6 = DownBlock(256, 256, noise_dim)\n",
    "        self.down7 = DownBlock(256, 256, noise_dim)\n",
    "        \n",
    "        # Up blocks\n",
    "        self.up1 = UpBlock(256, 256, noise_dim)\n",
    "        self.up2 = UpBlock(256, 256, noise_dim)\n",
    "        self.up3 = UpBlock(256, 256, noise_dim)\n",
    "        self.up4 = UpBlock(256, 256, noise_dim)\n",
    "        self.up5 = UpBlock(256, 128, noise_dim)\n",
    "        self.up6 = UpBlock(256, 64, noise_dim)\n",
    "        self.final_conv = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        \n",
    "        # FC layers for noise encoding\n",
    "        self.fc1 = nn.Linear(noise_dim, noise_dim)\n",
    "        self.fc2 = nn.Linear(noise_dim, noise_dim)\n",
    "\n",
    "    def forward(self, creases, noise):\n",
    "        # Encode noise\n",
    "        w = F.relu(self.fc1(noise))\n",
    "        w = F.relu(self.fc2(w))\n",
    "        \n",
    "        # Initial convolution\n",
    "        x = F.relu(self.initial_conv(creases))\n",
    "        \n",
    "        # Down-sampling\n",
    "        d1 = self.down1(x, w)\n",
    "        d2 = self.down2(d1, w)\n",
    "        d3 = self.down3(d2, w)\n",
    "        d4 = self.down4(d3, w)\n",
    "        d5 = self.down5(d4, w)\n",
    "        d6 = self.down6(d5, w)\n",
    "        d7 = self.down7(d6, w)\n",
    "        \n",
    "        # Up-sampling with skip connections\n",
    "        u1 = self.up1(d7, w, d6)\n",
    "        u2 = self.up2(u1, w, d5)\n",
    "        u3 = self.up3(u2, w, d4)\n",
    "        u4 = self.up4(u3, w, d3)\n",
    "        u5 = self.up5(u4, w, d2)\n",
    "        u6_input = torch.cat((u5, d1), dim=1)\n",
    "        u6 = self.up6(u6_input, w)\n",
    "        \n",
    "        # Final convolution\n",
    "        gen = torch.tanh(self.final_conv(u6))\n",
    "        return gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78edb6-73e3-4a22-b3d0-1b0d6cb0dd84",
   "metadata": {},
   "source": [
    "### Encoder (E) Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b8e40d5-1844-47e3-8628-e838483d7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, negative_slope=0.2):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # First sequence: BN -> LReLU -> Conv\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.lrelu1 = nn.LeakyReLU(negative_slope, inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        \n",
    "        # Second sequence: BN -> LReLU -> Conv\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.lrelu2 = nn.LeakyReLU(negative_slope, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # AvgPool layer\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2) if stride == 2 else nn.Identity()\n",
    "        \n",
    "        # Identity (skip connection) to match dimensions if stride > 1 or channel mismatch\n",
    "        self.identity = nn.Identity()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.identity = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)  # Skip connection\n",
    "        \n",
    "        # Forward path\n",
    "        out = self.bn1(x)\n",
    "        out = self.lrelu1(out)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = self.lrelu2(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        # Apply AvgPool to the output\n",
    "        out = self.avg_pool(out)\n",
    "        \n",
    "        # Add the skip connection\n",
    "        out += identity\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c237cd61-00cb-433c-9e03-940892f038f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  # Output size: 128x128x64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks with downsampling as specified\n",
    "        self.residual_block1 = ResidualBlock(64, 128, stride=2)   # Output size: 64x64x128\n",
    "        self.residual_block2 = ResidualBlock(128, 192, stride=2)  # Output size: 32x32x192\n",
    "        self.residual_block3 = ResidualBlock(192, 256, stride=2)  # Output size: 16x16x256\n",
    "        self.residual_block4 = ResidualBlock(256, 256, stride=2)  # Output size: 8x8x256\n",
    "        \n",
    "        # Fully connected layer to produce 1x8 output\n",
    "        self.fc = nn.Linear(256 * 8 * 8, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)              # Initial convolution\n",
    "        x = self.residual_block1(x)           # Residual block 1\n",
    "        x = self.residual_block2(x)           # Residual block 2\n",
    "        x = self.residual_block3(x)           # Residual block 3\n",
    "        x = self.residual_block4(x)           # Residual block 4\n",
    "        \n",
    "        x = x.view(x.size(0), -1)             # Flatten the output\n",
    "        enc = self.fc(x)                        # Fully connected layer to get 1x8 output\n",
    "        return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4b61c-3188-476b-82d2-0253365b1ae6",
   "metadata": {},
   "source": [
    "### Discriminator (D) Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16575dc9-7a79-4b1f-8cb3-9326780e8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "# Attention mechanism (from RLOC Discriminator)\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.attention(x)\n",
    "\n",
    "# RLOC Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Convolutional layers with spectral normalization\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1))  # Input: 256x256x3 -> Output: 128x128x64\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1))  # Output: 64x64x128\n",
    "        self.conv3 = spectral_norm(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1))  # Output: 32x32x256\n",
    "        self.attention = AttentionModule(256)  # Attention applied to 256 feature maps\n",
    "        self.fc = nn.Linear(256 * 32 * 32, 1)  # Flattened size from 32x32x256 -> 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)  # Leaky ReLU after conv1\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2)  # Leaky ReLU after conv2\n",
    "        x = F.leaky_relu(self.conv3(x), 0.2)  # Leaky ReLU after conv3\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        x = self.attention(x)  # Attention-modulated features\n",
    "\n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 256*32*32)\n",
    "\n",
    "        return torch.sigmoid(self.fc(x))  # Output: 1 value (real or fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0b331-c30a-40ac-86dd-592f8ea011aa",
   "metadata": {},
   "source": [
    "### ID-Aware Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5cf2686-52f5-4dbb-99af-7e1c27c8d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_aware_loss(DMB, B0, B00):\n",
    "    B0_features = DMB(B0)\n",
    "    B00_features = DMB(B00)\n",
    "    cosine_similarity = F.cosine_similarity(B0_features, B00_features, dim=1)\n",
    "    return 1 - cosine_similarity.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2e183f-e72a-4688-9f8e-37612686b6e8",
   "metadata": {},
   "source": [
    "### Total Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e4b5890-9926-4eaa-829e-b46baaa74fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(B, B0, mean, log_var, DMB, adversarial_loss):\n",
    "    lambda_1, lambda_2, lambda_kl, lambda_id = 10.0, 1.0, 0.01, 5.0\n",
    "    # L1 loss to ensure numerical similarity between B and B0\n",
    "    l1_loss = F.l1_loss(B, B0)\n",
    "    # KL divergence loss for regularizing the encoder to Gaussian noise\n",
    "    kl_div = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    # Adversarial loss using GAN for realistic image generation\n",
    "    adv_loss = adversarial_loss(B, B0)\n",
    "    # ID-aware loss to enforce identity consistency in generated images\n",
    "    id_loss_value = id_aware_loss(DMB, B0)\n",
    "\n",
    "    # Total loss \n",
    "    return lambda_1 * l1_loss + lambda_2 * adv_loss + lambda_kl * kl_div + lambda_id * id_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13978259-c031-495e-b48d-0633087d1e39",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d37ae-3d75-4563-a6f4-2b14d95c1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the custom dataset class\n",
    "class IITDPalmprintDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = os.path.join(root_dir, 'segmented')  # Use only 'segmented' folder\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "\n",
    "        # Collect image paths from 'segmented' folder (left and right subfolders)\n",
    "        for subfolder in ['left', 'right']:\n",
    "            folder_path = os.path.join(self.root_dir, subfolder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for img_file in os.listdir(folder_path):\n",
    "                    if img_file.endswith('.bmp'):  # IITD segmented images are in .bmp format\n",
    "                        self.image_paths.append(os.path.join(folder_path, img_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Return the same image twice for training purposes\n",
    "        return image, image  # (real_palmprint, palm_creases)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),  # Resize to match model input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize to [-1, 1] range\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"IITD Palmprint V1\"  # Replace with actual path to dataset\n",
    "dataset = IITDPalmprintDataset(root_dir=dataset_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Model definitions (assuming gen, enc, D are defined and initialized)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gen = Generator().to(device)\n",
    "enc = Encoder().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 60\n",
    "batch_size = 16\n",
    "latent_dim = 100\n",
    "learning_rate_G = 0.0002\n",
    "learning_rate_D = 0.0001\n",
    "L1_weight = 10.0\n",
    "LD_weight = 0.1\n",
    "LKL_weight = 0.01\n",
    "LID_weight = 5.0\n",
    "\n",
    "# Optimizers with independent learning rates for G and D\n",
    "optimizer_G = optim.Adam(gen.parameters(), lr=learning_rate_G, betas=(0.5, 0.99))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=learning_rate_D, betas=(0.5, 0.99))\n",
    "\n",
    "# Define linear decay function for learning rate\n",
    "def linear_decay(epoch, start_epoch=30, end_epoch=60, start_lr=learning_rate_G, end_lr=1e-8):\n",
    "    if epoch < start_epoch:\n",
    "        return 1.0\n",
    "    decay_factor = (epoch - start_epoch) / (end_epoch - start_epoch)\n",
    "    return max(1 - decay_factor, end_lr / start_lr)\n",
    "\n",
    "# Assign the linear decay scheduler to both optimizers\n",
    "scheduler_G = optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=linear_decay)\n",
    "scheduler_D = optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=linear_decay)\n",
    "\n",
    "# Loss functions\n",
    "criterion_L1 = nn.L1Loss()\n",
    "criterion_BCE = nn.BCEWithLogitsLoss()\n",
    "criterion_KL = lambda mean, log_var: -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "criterion_ID = nn.L1Loss()  # Placeholder for ID-aware loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_palmprint, palm_creases) in enumerate(dataloader):\n",
    "        real_palmprint = real_palmprint.to(device)\n",
    "        palm_creases = palm_creases.to(device)\n",
    "\n",
    "        # =================== Train Discriminator ===================\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Real images\n",
    "        real_labels = torch.ones((real_palmprint.size(0), 1), device=device)\n",
    "        fake_labels = torch.zeros((real_palmprint.size(0), 1), device=device)\n",
    "        \n",
    "        real_output = D(real_palmprint)\n",
    "        d_real_loss = criterion_BCE(real_output, real_labels)\n",
    "\n",
    "        # Fake images\n",
    "        latent_noise = torch.randn(real_palmprint.size(0), latent_dim, device=device)\n",
    "        fake_palmprint = gen(latent_noise, palm_creases).detach()  # Detach to avoid training gen during D training\n",
    "        fake_output = D(fake_palmprint)\n",
    "        d_fake_loss = criterion_BCE(fake_output, fake_labels)\n",
    "\n",
    "        # Total Discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) * LD_weight\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # =================== Train Generator and Encoder ===================\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generate fake palmprint\n",
    "        latent_noise = torch.randn(real_palmprint.size(0), latent_dim, device=device)\n",
    "        encoded_noise = enc(real_palmprint)  # Encode real palmprint to latent noise\n",
    "        fake_palmprint = gen(latent_noise, palm_creases)  # Generate palmprint from noise\n",
    "\n",
    "        # Compute various losses\n",
    "        L1_loss = L1_weight * criterion_L1(fake_palmprint, real_palmprint)\n",
    "        adv_loss = LD_weight * criterion_BCE(D(fake_palmprint), real_labels)  # Encourage gen to fool D\n",
    "        KL_loss = LKL_weight * criterion_KL(encoded_noise.mean, encoded_noise.log_var)\n",
    "\n",
    "        # ID-aware loss: Ensure ID consistency for generated samples\n",
    "        fake_palmprint_2 = gen(torch.randn(real_palmprint.size(0), latent_dim, device=device), palm_creases)\n",
    "        ID_loss = LID_weight * criterion_ID(fake_palmprint, fake_palmprint_2)\n",
    "\n",
    "        # Total generator loss\n",
    "        total_loss_G = L1_loss + adv_loss + KL_loss + ID_loss\n",
    "        total_loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i}], D Loss: {d_loss.item():.4f}, G Loss: {total_loss_G.item():.4f}, \"\n",
    "                  f\"L1: {L1_loss.item():.4f}, Adv: {adv_loss.item():.4f}, KL: {KL_loss.item():.4f}, ID: {ID_loss.item():.4f}\")\n",
    "\n",
    "    # Update learning rates\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d86af9d-41c6-4c39-9b69-1a74a0db406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pseudo_palmprint(synthetic_palm_creases, batch_size=16):\n",
    "    gen.eval()  # Set the generator to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        latent_noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "        generated_palmprint = gen(latent_noise, synthetic_palm_creases)\n",
    "    return generated_palmprint\n",
    "\n",
    "# Example usage\n",
    "# Assume `synthetic_creases_loader` is a DataLoader for synthetic creases\n",
    "for synthetic_palm_creases in synthetic_creases_loader:\n",
    "    synthetic_palm_creases = synthetic_palm_creases.to(device)\n",
    "    pseudo_palmprints = generate_pseudo_palmprint(synthetic_palm_creases, batch_size=synthetic_palm_creases.size(0))\n",
    "    \n",
    "    # Save or further process `pseudo_palmprints` as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64d80c-bee0-4ce4-9048-1cb596312866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
